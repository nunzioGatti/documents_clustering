{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling is an unsupervised learning approach to clustering documents, to discover topics based on their contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will walk through two different approaches for topic modeling, and compare their results. These approaches are LDA (Latent Derilicht Analysis), and NMF (Non-negative Matrix factorization). Let’s talk about each of these before we move onto code. We will look at their definitions, and some basic math that describe how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA, or Latent Derelicht Analysis is a probabilistic model, and to obtain cluster assignments, it uses two probability values: P( word | topics) and P( topics | documents). These values are calculated based on an initial random assignment, after which they are repeated for each word in each document, to decide their topic assignment. In an iterative procedure, these probabilities are calculated multiple times, until the convergence of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-negative Matrix Factorization is a Linear-algeabreic model, that factors high-dimensional vectors into a low-dimensionality representation. Similar to Principal component analysis (PCA), NMF takes advantage of the fact that the vectors are non-negative. By factoring them into the lower-dimensional form, NMF forces the coefficients to also be non-negative.\n",
    "Given the original matrix A, we can obtain two matrices W and H, such that A= WH. \n",
    "\n",
    "NMF has an inherent clustering property, such that W and H represent the following information about A:\n",
    "\n",
    "-A (Document-word matrix) — input that contains which words appear in which documents.\n",
    "\n",
    "-W (Basis vectors) — the topics (clusters) discovered from the documents.\n",
    "\n",
    "-H (Coefficient matrix) — the membership weights for the topics in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will apply topic modeling on the ABC Millions Headlines dataset (published on Kaggle recently: https://www.kaggle.com/therohk/million-headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import scipy as sp;\n",
    "import sklearn;\n",
    "import sys;\n",
    "from nltk.corpus import stopwords;\n",
    "import nltk;\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora;\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\n",
    "from sklearn.decomposition import NMF;\n",
    "from sklearn.preprocessing import normalize;\n",
    "import pickle;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 7A75-B79E\n",
      "\n",
      " Directory of C:\\Users\\CAMNG3\\documents_clustering\n",
      "\n",
      "21/10/2019  11:32    <DIR>          .\n",
      "21/10/2019  11:32    <DIR>          ..\n",
      "21/10/2019  11:05    <DIR>          .ipynb_checkpoints\n",
      "21/10/2019  11:27        55,392,904 abcnews-date-text.csv\n",
      "21/10/2019  10:52                24 README.md\n",
      "21/10/2019  11:32            10,090 Topic_modeling.ipynb\n",
      "               3 File(s)     55,403,018 bytes\n",
      "               3 Dir(s)  404,868,915,200 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"abcnews-date-text.csv\", warn_bad_lines=True,error_bad_lines=False)\n",
    "data_text = data[['headline_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103663"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove stopwords first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text.sample(frac=0.01, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11037"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consiering the big amount of data we can use spark to speed up everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "import pyspark as sp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sql function on a SQLContext enables applications to run SQL queries\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "session= SparkSession.builder.appName('pandasToSparkDF').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECIDE IN HOW MANY PARTITIONS WE WANT TO SPLIT THE DATAFRAME\n",
    "num_part = sc.parallelize(data_text['headline_text'],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas to dataframe\n",
    "df = spark.createDataFrame(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       headline_text|\n",
      "+--------------------+\n",
      "|millions of dolla...|\n",
      "|burrow and fannin...|\n",
      "|teens rampage in ...|\n",
      "|owners urged to p...|\n",
      "|australians urged...|\n",
      "|us markets fail t...|\n",
      "|industrial relati...|\n",
      "|indigenous missin...|\n",
      "|police cite falli...|\n",
      "|long jail term so...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_df=df.select('headline_text').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009996891021728516\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "num_part = num_part.map(lambda x: [[word for word in x.split() if word not in stopwords.words()]])\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['millions', 'dollars', 'could', 'saved', 'less', 'invasive', 'surge']],\n",
       " [['burrow', 'fanning', 'survive', 'elimination', 'heats', 'fiji', 'pro']],\n",
       " [['teens', 'rampage', 'vacant', 'house']]]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_part.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rdd to dataframe\n",
    "s = sqlContext.createDataFrame(num_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  _1|\n",
      "+--------------------+\n",
      "|[millions, dollar...|\n",
      "|[burrow, fanning,...|\n",
      "|[teens, rampage, ...|\n",
      "|[owners, urged, p...|\n",
      "|[australians, urg...|\n",
      "|[us, markets, fai...|\n",
      "|[industrial, rela...|\n",
      "|[indigenous, miss...|\n",
      "|[police, falling,...|\n",
      "|[long, jail, term...|\n",
      "|[three, trapped, ...|\n",
      "|[racq, calls, roa...|\n",
      "|[woman, doused, p...|\n",
      "|[dish, celebrates...|\n",
      "|[philippines, flo...|\n",
      "|[flying, clydesdale]|\n",
      "|[scientist, surpr...|\n",
      "|[taxi, driver, gr...|\n",
      "|[water, ski, prop...|\n",
      "|[wikimedia, boss,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = s.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11037"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data because it took very long \n",
    "data_pd.to_csv(\"trial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# data_text['headline_text'] = data_text['headline_text'].apply(lambda x : [word for word in x.split() if word not in stopwords.words()])\n",
    "# end = time.time()\n",
    "# print(end-start)\n",
    "\n",
    "#after 30 seconds was still running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the wrods as an array for lda input\n",
    "train_headlines = [value[0] for value in data_pd.iloc[0:].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13318"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of unique words\n",
    "from itertools import chain\n",
    "\n",
    "docs_temp = [word for elem in train_headlines for word in elem]\n",
    "len(set(docs_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the number of Topics we need to cluster:\n",
    "num_topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the gensim library for LDA. First, we obtain a id-2-word dictionary. For each headline, we will use the dictionary to obtain a mapping of the word id to their word counts. The LDA model uses both of these mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(train_headlines)\n",
    "\n",
    "# To convert documents to vectors, we’ll use a document\n",
    "#representation called bag-of-words. In this representation, \n",
    "#each document is represented by one vector where each vector \n",
    "#element represents a question-answer pair, in the style of:\n",
    "# “How many times does the word system appear in the document? Once.”\n",
    "# ex ['alfa','beta'] => (34,1),(35,1)\n",
    "#    ['alfa','alfa'] => (34,2)\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in train_headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(14, 1), (15, 1), (16, 1), (17, 1)],\n",
       " [(18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)],\n",
       " [(23, 1), (24, 1), (25, 1), (26, 1)]]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
